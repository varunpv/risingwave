# This file is automatically generated. See `src/frontend/planner_test/README.md` for more information.
- sql: |
    select * from unnest(array[1,2,3]) WITH ORDINALITY;
  batch_plan: |-
    BatchProject { exprs: [Unnest(ARRAY[1, 2, 3]:List(Int32)), (projected_row_id + 1:Int64) as $expr1] }
    └─BatchProjectSet { select_list: [Unnest(ARRAY[1, 2, 3]:List(Int32))] }
      └─BatchValues { rows: [[]] }
  stream_plan: |-
    StreamMaterialize { columns: [unnest, ordinality, _row_id(hidden), projected_row_id(hidden)], stream_key: [_row_id, projected_row_id], pk_columns: [_row_id, projected_row_id], pk_conflict: NoCheck }
    └─StreamProject { exprs: [Unnest(ARRAY[1, 2, 3]:List(Int32)), (projected_row_id + 1:Int64) as $expr1, _row_id, projected_row_id] }
      └─StreamProjectSet { select_list: [Unnest(ARRAY[1, 2, 3]:List(Int32)), $0] }
        └─StreamValues { rows: [[0:Int64]] }
- sql: |
    create table t(x int , arr int[]);
    select * from t cross join unnest(arr) WITH ORDINALITY;
  batch_plan: |-
    BatchExchange { order: [], dist: Single }
    └─BatchProject { exprs: [t.x, t.arr, Unnest($0), (projected_row_id + 1:Int64) as $expr1] }
      └─BatchHashJoin { type: Inner, predicate: t.arr IS NOT DISTINCT FROM t.arr, output: all }
        ├─BatchExchange { order: [], dist: HashShard(t.arr) }
        │ └─BatchScan { table: t, columns: [t.x, t.arr], distribution: SomeShard }
        └─BatchProjectSet { select_list: [$0, Unnest($0)] }
          └─BatchHashAgg { group_key: [t.arr], aggs: [] }
            └─BatchExchange { order: [], dist: HashShard(t.arr) }
              └─BatchScan { table: t, columns: [t.arr], distribution: SomeShard }
  stream_plan: |-
    StreamMaterialize { columns: [x, arr, unnest, ordinality, t._row_id(hidden), projected_row_id(hidden)], stream_key: [t._row_id, projected_row_id, arr], pk_columns: [t._row_id, projected_row_id, arr], pk_conflict: NoCheck }
    └─StreamProject { exprs: [t.x, t.arr, Unnest($0), (projected_row_id + 1:Int64) as $expr1, t._row_id, projected_row_id] }
      └─StreamHashJoin { type: Inner, predicate: t.arr IS NOT DISTINCT FROM t.arr, output: [t.x, t.arr, projected_row_id, t.arr, Unnest($0), t._row_id] }
        ├─StreamExchange { dist: HashShard(t.arr) }
        │ └─StreamTableScan { table: t, columns: [t.x, t.arr, t._row_id], stream_scan_type: ArrangementBackfill, stream_key: [t._row_id], pk: [_row_id], dist: UpstreamHashShard(t._row_id) }
        └─StreamProjectSet { select_list: [$0, Unnest($0)] }
          └─StreamProject { exprs: [t.arr] }
            └─StreamHashAgg { group_key: [t.arr], aggs: [count] }
              └─StreamExchange { dist: HashShard(t.arr) }
                └─StreamTableScan { table: t, columns: [t.arr, t._row_id], stream_scan_type: ArrangementBackfill, stream_key: [t._row_id], pk: [_row_id], dist: UpstreamHashShard(t._row_id) }
- sql: |
    create table t(x int , arr int[]);
    select * from t cross join unnest(arr) WITH ORDINALITY as foo;
  batch_plan: |-
    BatchExchange { order: [], dist: Single }
    └─BatchProject { exprs: [t.x, t.arr, Unnest($0), (projected_row_id + 1:Int64) as $expr1] }
      └─BatchHashJoin { type: Inner, predicate: t.arr IS NOT DISTINCT FROM t.arr, output: all }
        ├─BatchExchange { order: [], dist: HashShard(t.arr) }
        │ └─BatchScan { table: t, columns: [t.x, t.arr], distribution: SomeShard }
        └─BatchProjectSet { select_list: [$0, Unnest($0)] }
          └─BatchHashAgg { group_key: [t.arr], aggs: [] }
            └─BatchExchange { order: [], dist: HashShard(t.arr) }
              └─BatchScan { table: t, columns: [t.arr], distribution: SomeShard }
  stream_plan: |-
    StreamMaterialize { columns: [x, arr, foo, ordinality, t._row_id(hidden), projected_row_id(hidden)], stream_key: [t._row_id, projected_row_id, arr], pk_columns: [t._row_id, projected_row_id, arr], pk_conflict: NoCheck }
    └─StreamProject { exprs: [t.x, t.arr, Unnest($0), (projected_row_id + 1:Int64) as $expr1, t._row_id, projected_row_id] }
      └─StreamHashJoin { type: Inner, predicate: t.arr IS NOT DISTINCT FROM t.arr, output: [t.x, t.arr, projected_row_id, t.arr, Unnest($0), t._row_id] }
        ├─StreamExchange { dist: HashShard(t.arr) }
        │ └─StreamTableScan { table: t, columns: [t.x, t.arr, t._row_id], stream_scan_type: ArrangementBackfill, stream_key: [t._row_id], pk: [_row_id], dist: UpstreamHashShard(t._row_id) }
        └─StreamProjectSet { select_list: [$0, Unnest($0)] }
          └─StreamProject { exprs: [t.arr] }
            └─StreamHashAgg { group_key: [t.arr], aggs: [count] }
              └─StreamExchange { dist: HashShard(t.arr) }
                └─StreamTableScan { table: t, columns: [t.arr, t._row_id], stream_scan_type: ArrangementBackfill, stream_key: [t._row_id], pk: [_row_id], dist: UpstreamHashShard(t._row_id) }
- sql: |
    create table t(x int , arr int[]);
    select * from t cross join unnest(arr) WITH ORDINALITY as foo(a);
  batch_plan: |-
    BatchExchange { order: [], dist: Single }
    └─BatchProject { exprs: [t.x, t.arr, Unnest($0), (projected_row_id + 1:Int64) as $expr1] }
      └─BatchHashJoin { type: Inner, predicate: t.arr IS NOT DISTINCT FROM t.arr, output: all }
        ├─BatchExchange { order: [], dist: HashShard(t.arr) }
        │ └─BatchScan { table: t, columns: [t.x, t.arr], distribution: SomeShard }
        └─BatchProjectSet { select_list: [$0, Unnest($0)] }
          └─BatchHashAgg { group_key: [t.arr], aggs: [] }
            └─BatchExchange { order: [], dist: HashShard(t.arr) }
              └─BatchScan { table: t, columns: [t.arr], distribution: SomeShard }
  stream_plan: |-
    StreamMaterialize { columns: [x, arr, a, ordinality, t._row_id(hidden), projected_row_id(hidden)], stream_key: [t._row_id, projected_row_id, arr], pk_columns: [t._row_id, projected_row_id, arr], pk_conflict: NoCheck }
    └─StreamProject { exprs: [t.x, t.arr, Unnest($0), (projected_row_id + 1:Int64) as $expr1, t._row_id, projected_row_id] }
      └─StreamHashJoin { type: Inner, predicate: t.arr IS NOT DISTINCT FROM t.arr, output: [t.x, t.arr, projected_row_id, t.arr, Unnest($0), t._row_id] }
        ├─StreamExchange { dist: HashShard(t.arr) }
        │ └─StreamTableScan { table: t, columns: [t.x, t.arr, t._row_id], stream_scan_type: ArrangementBackfill, stream_key: [t._row_id], pk: [_row_id], dist: UpstreamHashShard(t._row_id) }
        └─StreamProjectSet { select_list: [$0, Unnest($0)] }
          └─StreamProject { exprs: [t.arr] }
            └─StreamHashAgg { group_key: [t.arr], aggs: [count] }
              └─StreamExchange { dist: HashShard(t.arr) }
                └─StreamTableScan { table: t, columns: [t.arr, t._row_id], stream_scan_type: ArrangementBackfill, stream_key: [t._row_id], pk: [_row_id], dist: UpstreamHashShard(t._row_id) }
- sql: |
    create table t(x int , arr int[]);
    select * from t cross join unnest(arr) WITH ORDINALITY as foo(a,ord);
  batch_plan: |-
    BatchExchange { order: [], dist: Single }
    └─BatchProject { exprs: [t.x, t.arr, Unnest($0), (projected_row_id + 1:Int64) as $expr1] }
      └─BatchHashJoin { type: Inner, predicate: t.arr IS NOT DISTINCT FROM t.arr, output: all }
        ├─BatchExchange { order: [], dist: HashShard(t.arr) }
        │ └─BatchScan { table: t, columns: [t.x, t.arr], distribution: SomeShard }
        └─BatchProjectSet { select_list: [$0, Unnest($0)] }
          └─BatchHashAgg { group_key: [t.arr], aggs: [] }
            └─BatchExchange { order: [], dist: HashShard(t.arr) }
              └─BatchScan { table: t, columns: [t.arr], distribution: SomeShard }
  stream_plan: |-
    StreamMaterialize { columns: [x, arr, a, ord, t._row_id(hidden), projected_row_id(hidden)], stream_key: [t._row_id, projected_row_id, arr], pk_columns: [t._row_id, projected_row_id, arr], pk_conflict: NoCheck }
    └─StreamProject { exprs: [t.x, t.arr, Unnest($0), (projected_row_id + 1:Int64) as $expr1, t._row_id, projected_row_id] }
      └─StreamHashJoin { type: Inner, predicate: t.arr IS NOT DISTINCT FROM t.arr, output: [t.x, t.arr, projected_row_id, t.arr, Unnest($0), t._row_id] }
        ├─StreamExchange { dist: HashShard(t.arr) }
        │ └─StreamTableScan { table: t, columns: [t.x, t.arr, t._row_id], stream_scan_type: ArrangementBackfill, stream_key: [t._row_id], pk: [_row_id], dist: UpstreamHashShard(t._row_id) }
        └─StreamProjectSet { select_list: [$0, Unnest($0)] }
          └─StreamProject { exprs: [t.arr] }
            └─StreamHashAgg { group_key: [t.arr], aggs: [count] }
              └─StreamExchange { dist: HashShard(t.arr) }
                └─StreamTableScan { table: t, columns: [t.arr, t._row_id], stream_scan_type: ArrangementBackfill, stream_key: [t._row_id], pk: [_row_id], dist: UpstreamHashShard(t._row_id) }
- name: use alias columns explicitlity
  sql: |
    create table t(x int , arr int[]);
    select x, arr, a, ord from t cross join unnest(arr) WITH ORDINALITY as foo(a,ord);
  batch_plan: |-
    BatchExchange { order: [], dist: Single }
    └─BatchProject { exprs: [t.x, t.arr, Unnest($0), (projected_row_id + 1:Int64) as $expr1] }
      └─BatchHashJoin { type: Inner, predicate: t.arr IS NOT DISTINCT FROM t.arr, output: all }
        ├─BatchExchange { order: [], dist: HashShard(t.arr) }
        │ └─BatchScan { table: t, columns: [t.x, t.arr], distribution: SomeShard }
        └─BatchProjectSet { select_list: [$0, Unnest($0)] }
          └─BatchHashAgg { group_key: [t.arr], aggs: [] }
            └─BatchExchange { order: [], dist: HashShard(t.arr) }
              └─BatchScan { table: t, columns: [t.arr], distribution: SomeShard }
  stream_plan: |-
    StreamMaterialize { columns: [x, arr, a, ord, t._row_id(hidden), projected_row_id(hidden)], stream_key: [t._row_id, projected_row_id, arr], pk_columns: [t._row_id, projected_row_id, arr], pk_conflict: NoCheck }
    └─StreamProject { exprs: [t.x, t.arr, Unnest($0), (projected_row_id + 1:Int64) as $expr1, t._row_id, projected_row_id] }
      └─StreamHashJoin { type: Inner, predicate: t.arr IS NOT DISTINCT FROM t.arr, output: [t.x, t.arr, projected_row_id, t.arr, Unnest($0), t._row_id] }
        ├─StreamExchange { dist: HashShard(t.arr) }
        │ └─StreamTableScan { table: t, columns: [t.x, t.arr, t._row_id], stream_scan_type: ArrangementBackfill, stream_key: [t._row_id], pk: [_row_id], dist: UpstreamHashShard(t._row_id) }
        └─StreamProjectSet { select_list: [$0, Unnest($0)] }
          └─StreamProject { exprs: [t.arr] }
            └─StreamHashAgg { group_key: [t.arr], aggs: [count] }
              └─StreamExchange { dist: HashShard(t.arr) }
                └─StreamTableScan { table: t, columns: [t.arr, t._row_id], stream_scan_type: ArrangementBackfill, stream_key: [t._row_id], pk: [_row_id], dist: UpstreamHashShard(t._row_id) }
- sql: |
    create table t(x int , arr int[]);
    select * from t cross join unnest(arr) WITH ORDINALITY as foo(a,ord,bar);
  binder_error: 'Bind error: table "foo" has 2 columns available but 3 column aliases specified'
- sql: |
    create table t(x int , arr int[]);
    select * from t cross join unnest(arr) WITH ORDINALITY, unnest(arr) WITH ORDINALITY AS unnest_2(arr_2,ordinality_2);
  batch_plan: |-
    BatchExchange { order: [], dist: Single }
    └─BatchProject { exprs: [t.x, t.arr, Unnest($0), (projected_row_id + 1:Int64) as $expr1, Unnest($0), (projected_row_id + 1:Int64) as $expr2] }
      └─BatchHashJoin { type: Inner, predicate: t.arr IS NOT DISTINCT FROM t.arr, output: all }
        ├─BatchHashJoin { type: Inner, predicate: t.arr IS NOT DISTINCT FROM t.arr, output: all }
        │ ├─BatchExchange { order: [], dist: HashShard(t.arr) }
        │ │ └─BatchScan { table: t, columns: [t.x, t.arr], distribution: SomeShard }
        │ └─BatchProjectSet { select_list: [$0, Unnest($0)] }
        │   └─BatchHashAgg { group_key: [t.arr], aggs: [] }
        │     └─BatchExchange { order: [], dist: HashShard(t.arr) }
        │       └─BatchScan { table: t, columns: [t.arr], distribution: SomeShard }
        └─BatchProjectSet { select_list: [$0, Unnest($0)] }
          └─BatchHashAgg { group_key: [t.arr], aggs: [] }
            └─BatchHashJoin { type: Inner, predicate: t.arr IS NOT DISTINCT FROM t.arr, output: [t.arr] }
              ├─BatchExchange { order: [], dist: HashShard(t.arr) }
              │ └─BatchScan { table: t, columns: [t.arr], distribution: SomeShard }
              └─BatchProjectSet { select_list: [$0, Unnest($0)] }
                └─BatchHashAgg { group_key: [t.arr], aggs: [] }
                  └─BatchExchange { order: [], dist: HashShard(t.arr) }
                    └─BatchScan { table: t, columns: [t.arr], distribution: SomeShard }
  stream_plan: |-
    StreamMaterialize { columns: [x, arr, unnest, ordinality, arr_2, ordinality_2, t._row_id(hidden), projected_row_id(hidden), projected_row_id#1(hidden)], stream_key: [t._row_id, projected_row_id, arr, projected_row_id#1], pk_columns: [t._row_id, projected_row_id, arr, projected_row_id#1], pk_conflict: NoCheck }
    └─StreamProject { exprs: [t.x, t.arr, Unnest($0), $expr1, Unnest($0), (projected_row_id + 1:Int64) as $expr2, t._row_id, projected_row_id, projected_row_id] }
      └─StreamHashJoin { type: Inner, predicate: t.arr IS NOT DISTINCT FROM t.arr, output: [t.x, t.arr, Unnest($0), $expr1, projected_row_id, t.arr, Unnest($0), t._row_id, projected_row_id] }
        ├─StreamShare { id: 8 }
        │ └─StreamProject { exprs: [t.x, t.arr, Unnest($0), (projected_row_id + 1:Int64) as $expr1, t._row_id, projected_row_id] }
        │   └─StreamHashJoin { type: Inner, predicate: t.arr IS NOT DISTINCT FROM t.arr, output: [t.x, t.arr, projected_row_id, t.arr, Unnest($0), t._row_id] }
        │     ├─StreamExchange { dist: HashShard(t.arr) }
        │     │ └─StreamTableScan { table: t, columns: [t.x, t.arr, t._row_id], stream_scan_type: ArrangementBackfill, stream_key: [t._row_id], pk: [_row_id], dist: UpstreamHashShard(t._row_id) }
        │     └─StreamProjectSet { select_list: [$0, Unnest($0)] }
        │       └─StreamProject { exprs: [t.arr] }
        │         └─StreamHashAgg { group_key: [t.arr], aggs: [count] }
        │           └─StreamExchange { dist: HashShard(t.arr) }
        │             └─StreamTableScan { table: t, columns: [t.arr, t._row_id], stream_scan_type: ArrangementBackfill, stream_key: [t._row_id], pk: [_row_id], dist: UpstreamHashShard(t._row_id) }
        └─StreamProjectSet { select_list: [$0, Unnest($0)] }
          └─StreamProject { exprs: [t.arr] }
            └─StreamHashAgg { group_key: [t.arr], aggs: [count] }
              └─StreamShare { id: 8 }
                └─StreamProject { exprs: [t.x, t.arr, Unnest($0), (projected_row_id + 1:Int64) as $expr1, t._row_id, projected_row_id] }
                  └─StreamHashJoin { type: Inner, predicate: t.arr IS NOT DISTINCT FROM t.arr, output: [t.x, t.arr, projected_row_id, t.arr, Unnest($0), t._row_id] }
                    ├─StreamExchange { dist: HashShard(t.arr) }
                    │ └─StreamTableScan { table: t, columns: [t.x, t.arr, t._row_id], stream_scan_type: ArrangementBackfill, stream_key: [t._row_id], pk: [_row_id], dist: UpstreamHashShard(t._row_id) }
                    └─StreamProjectSet { select_list: [$0, Unnest($0)] }
                      └─StreamProject { exprs: [t.arr] }
                        └─StreamHashAgg { group_key: [t.arr], aggs: [count] }
                          └─StreamExchange { dist: HashShard(t.arr) }
                            └─StreamTableScan { table: t, columns: [t.arr, t._row_id], stream_scan_type: ArrangementBackfill, stream_key: [t._row_id], pk: [_row_id], dist: UpstreamHashShard(t._row_id) }
- sql: |
    select * from abs(1) WITH ORDINALITY;
  batch_plan: |-
    BatchProject { exprs: [, 1:Int64] }
    └─BatchValues { rows: [[1:Int32]] }
  stream_plan: |-
    StreamMaterialize { columns: [abs, ordinality, _row_id(hidden)], stream_key: [_row_id], pk_columns: [_row_id], pk_conflict: NoCheck }
    └─StreamProject { exprs: [, 1:Int64, _row_id] }
      └─StreamValues { rows: [[1:Int32, 0:Int64]] }
- sql: |
    create table t(x int , arr int[]);
    select * from t, abs(x) WITH ORDINALITY;
  batch_plan: |-
    BatchExchange { order: [], dist: Single }
    └─BatchProject { exprs: [t.x, t.arr, Abs(t.x) as $expr1, 1:Int64] }
      └─BatchHashJoin { type: Inner, predicate: t.x IS NOT DISTINCT FROM t.x, output: all }
        ├─BatchExchange { order: [], dist: HashShard(t.x) }
        │ └─BatchScan { table: t, columns: [t.x, t.arr], distribution: SomeShard }
        └─BatchExchange { order: [], dist: HashShard(t.x) }
          └─BatchNestedLoopJoin { type: Inner, predicate: true, output: all }
            ├─BatchExchange { order: [], dist: Single }
            │ └─BatchHashAgg { group_key: [t.x], aggs: [] }
            │   └─BatchExchange { order: [], dist: HashShard(t.x) }
            │     └─BatchScan { table: t, columns: [t.x], distribution: SomeShard }
            └─BatchValues { rows: [[]] }
  stream_error: |-
    Not supported: streaming nested-loop join
    HINT: The non-equal join in the query requires a nested-loop join executor, which could be very expensive to run. Consider rewriting the query to use dynamic filter as a substitute if possible.
    See also: https://docs.risingwave.com/docs/current/sql-pattern-dynamic-filters/
